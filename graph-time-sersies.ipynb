{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\n========================================\nFAIR BASELINE BENCHMARK - Q1 TIME SERIES\n========================================\nAll models constrained to ~20,000 parameters\nFast execution on Kaggle P100 / CUDA\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\nimport pandas as pd\nimport time\nimport pickle\nfrom pathlib import Path\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ==============================================================================\n# CONFIGURATION\n# ==============================================================================\nCONFIG = {\n    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu',\n    'BATCH_SIZE': 2048, # Large batch for speed on simple data\n    'EPOCHS': 20,\n    'LR': 0.003,\n    'SEQ_LEN': 96,\n    'PRED_LEN': 24,\n    'TARGET_PARAMS': 20000, \n    'SEED': 42,\n    'SAVE_DIR': Path('./benchmark_results')\n}\n\nCONFIG['SAVE_DIR'].mkdir(exist_ok=True)\ntorch.manual_seed(CONFIG['SEED'])\nnp.random.seed(CONFIG['SEED'])\n\nprint(f\"ðŸš€ Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\nprint(f\"ðŸŽ¯ Target: ~{CONFIG['TARGET_PARAMS']:,} parameters per model\\n\")\n\n# ==============================================================================\n# DATASETS\n# ==============================================================================\n\ndef load_electricity(n=15000):\n    print(\"Loading Electricity...\", end=' ')\n    t = np.arange(n)\n    data = np.zeros((n, 12))\n    for i in range(12):\n        daily = np.sin(t * 2*np.pi / 24 + i*0.5)\n        weekly = 1.5 * np.sin(t * 2*np.pi / 168)\n        trend = np.linspace(0, 2, n)\n        noise = np.random.normal(0, 0.3, n)\n        data[:, i] = daily + weekly + trend + noise\n    scaler = StandardScaler()\n    print(\"âœ“\")\n    return scaler.fit_transform(data).astype(np.float32)\n\ndef load_traffic(n=15000):\n    # Function kept for structure, but not used in datasets dictionary\n    print(\"Loading Traffic...\", end=' ')\n    t = np.arange(n)\n    data = np.zeros((n, 8))\n    for i in range(8):\n        morning = 2 * np.exp(-((t % 24 - 8)**2) / 8)\n        evening = 2 * np.exp(-((t % 24 - 18)**2) / 8)\n        weekend = 0.5 * np.sin(t * 2*np.pi / 168)\n        baseline = 1 + 0.3 * np.sin(t * 2*np.pi / (24*365))\n        noise = np.random.gamma(2, 0.2, n)\n        data[:, i] = baseline * (1 + morning + evening) * (1 + weekend) + noise\n    scaler = StandardScaler()\n    print(\"âœ“\")\n    return scaler.fit_transform(data).astype(np.float32)\n\ndef create_dataset(data, seq_len, pred_len):\n    X, y = [], []\n    for i in range(len(data) - seq_len - pred_len):\n        X.append(data[i:i+seq_len])\n        y.append(data[i+seq_len:i+seq_len+pred_len, 0]) # Target: First column only\n    \n    X, y = np.array(X), np.array(y)\n    split = int(0.7 * len(X))\n    val_split = int(0.85 * len(X))\n    \n    return (\n        (X[:split], y[:split]),\n        (X[split:val_split], y[split:val_split]),\n        (X[val_split:], y[val_split:])\n    )\n\n# ETT removed from here\n# Traffic removed from here as requested\ndatasets = {\n    'Electricity': load_electricity()\n}\n\n# ==============================================================================\n# MODELS (~20k params)\n# ==============================================================================\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nclass LSTM_Model(nn.Module):\n    def __init__(self, input_dim, pred_len=24):\n        super().__init__()\n        # Tuned to ~20k\n        hidden = int(np.sqrt(20000 / (4 * input_dim + 1)))\n        self.lstm = nn.LSTM(input_dim, hidden, 2, batch_first=True, dropout=0.1)\n        self.fc = nn.Linear(hidden, pred_len)\n    \n    def forward(self, x):\n        out, _ = self.lstm(x)\n        return self.fc(out[:, -1, :])\n\nclass GRU_Model(nn.Module):\n    def __init__(self, input_dim, pred_len=24):\n        super().__init__()\n        hidden = int(np.sqrt(20000 / (3 * input_dim + 1)))\n        self.gru = nn.GRU(input_dim, hidden, 2, batch_first=True, dropout=0.1)\n        self.fc = nn.Linear(hidden, pred_len)\n    \n    def forward(self, x):\n        out, _ = self.gru(x)\n        return self.fc(out[:, -1, :])\n\nclass Transformer_Model(nn.Module):\n    def __init__(self, input_dim, pred_len=24):\n        super().__init__()\n        d_model = 32\n        self.input_proj = nn.Linear(input_dim, d_model)\n        self.pos_encoder = nn.Parameter(torch.randn(1, 500, d_model) * 0.02)\n        \n        layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=4, dim_feedforward=d_model*2,\n            dropout=0.1, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(layer, num_layers=2)\n        self.fc = nn.Linear(d_model, pred_len)\n    \n    def forward(self, x):\n        x = self.input_proj(x) + self.pos_encoder[:, :x.size(1), :]\n        x = self.transformer(x)\n        return self.fc(x[:, -1, :])\n\nclass PatchTST_Model(nn.Module):\n    def __init__(self, input_dim, seq_len=96, pred_len=24):\n        super().__init__()\n        patch_len = 16\n        self.patch_len = patch_len\n        self.num_patches = seq_len // patch_len\n        \n        d_model = 32\n        self.patch_embed = nn.Linear(patch_len * input_dim, d_model)\n        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches, d_model) * 0.02)\n        \n        layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=4, dim_feedforward=d_model*2,\n            dropout=0.1, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(layer, num_layers=1)\n        self.head = nn.Linear(d_model * self.num_patches, pred_len)\n    \n    def forward(self, x):\n        B, T, F = x.shape\n        x = x.reshape(B, self.num_patches, self.patch_len * F)\n        x = self.patch_embed(x) + self.pos_embed\n        x = self.transformer(x)\n        return self.head(x.reshape(B, -1))\n\nclass FEDformer_Model(nn.Module):\n    def __init__(self, input_dim, pred_len=24):\n        super().__init__()\n        d_model = 32\n        self.input_proj = nn.Linear(input_dim, d_model)\n        self.freq_attn = nn.MultiheadAttention(d_model, num_heads=4, batch_first=True)\n        self.seasonal = nn.Linear(d_model, pred_len)\n        self.trend = nn.Linear(d_model, pred_len)\n    \n    def forward(self, x):\n        x = self.input_proj(x)\n        x_freq, _ = self.freq_attn(x, x, x)\n        return self.seasonal(x_freq[:, -1, :]) + self.trend(x[:, -1, :])\n\n# --- NEW MODELS ADDED BELOW ---\n\nclass TSMixer_Model(nn.Module):\n    def __init__(self, input_dim, seq_len=96, pred_len=24):\n        super().__init__()\n        # Tuned to fit ~20k\n        d_model = 16\n        self.input_lin = nn.Linear(input_dim, d_model)\n        \n        # 2 Mixing Layers\n        self.time_mix = nn.Sequential(\n            nn.Linear(seq_len, seq_len),\n            nn.ReLU(),\n            nn.Dropout(0.1)\n        )\n        self.feature_mix = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.ReLU(),\n            nn.Dropout(0.1)\n        )\n        \n        # Projection for first column output\n        self.head = nn.Linear(seq_len * d_model, pred_len)\n\n    def forward(self, x):\n        # x: [B, S, C]\n        x = self.input_lin(x) # [B, S, d]\n        \n        # Time Mixing\n        x_t = x.permute(0, 2, 1) # [B, d, S]\n        x_t = self.time_mix(x_t)\n        x = x + x_t.permute(0, 2, 1)\n        \n        # Feature Mixing\n        x_f = self.feature_mix(x)\n        x = x + x_f\n        \n        # Flatten and Project to (B, Pred)\n        x = x.reshape(x.size(0), -1)\n        return self.head(x)\n\nclass iTransformer_Model(nn.Module):\n    def __init__(self, input_dim, seq_len=96, pred_len=24):\n        super().__init__()\n        d_model = 32 # Embedding entire time series\n        self.embed = nn.Linear(seq_len, d_model)\n        \n        layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=4, dim_feedforward=64,\n            dropout=0.1, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(layer, num_layers=1)\n        \n        # Project channel 0 embedding to prediction\n        self.head = nn.Linear(d_model, pred_len)\n        \n    def forward(self, x):\n        # x: [B, S, C] -> Invert -> [B, C, S]\n        x = x.permute(0, 2, 1)\n        \n        # Embed time dim\n        x = self.embed(x) # [B, C, d_model]\n        \n        # Attention across variables\n        x = self.encoder(x)\n        \n        # We only want to predict the first variable (Column 0)\n        # So we take the 0th token\n        x_target = x[:, 0, :] # [B, d_model]\n        \n        return self.head(x_target)\n\nclass MTGNN_Model(nn.Module):\n    def __init__(self, input_dim, seq_len=96, pred_len=24):\n        super().__init__()\n        # Simplified Graph Convolution for strict parameter budget\n        c_out = 8\n        self.node_embeddings = nn.Parameter(torch.randn(input_dim, 4), requires_grad=True)\n        \n        self.gcn_lin = nn.Linear(c_out, c_out)\n        self.input_conv = nn.Conv1d(input_dim, c_out, kernel_size=1)\n        self.temporal_conv = nn.Conv1d(c_out, c_out, kernel_size=3, padding=1)\n        \n        # Output: Project from (c_out * seq_len) -> pred_len\n        self.head = nn.Linear(c_out * seq_len, pred_len)\n\n    def forward(self, x):\n        B, S, C = x.shape\n        \n        # 1. Learn Graph\n        adj = F.softmax(F.relu(torch.mm(self.node_embeddings, self.node_embeddings.T)), dim=1)\n        \n        # 2. Input Proj: [B, C, S] -> [B, 8, S]\n        # (Mixing channels into latent dims)\n        x_in = x.permute(0, 2, 1)\n        x_conv = self.input_conv(x_in) \n        \n        # 3. Graph Conv (Simplified)\n        # Usually GCN is (AXW). Here we approximate by mixing latent features\n        # We assume latent features capture node info.\n        # x_conv: [B, 8, S]\n        x_gcn = x_conv.permute(0, 2, 1) # [B, S, 8]\n        x_gcn = self.gcn_lin(x_gcn)     # [B, S, 8]\n        x_gcn = x_gcn.permute(0, 2, 1)  # [B, 8, S]\n        \n        # 4. Temporal Conv\n        x_out = F.relu(self.temporal_conv(x_conv + x_gcn))\n        \n        # 5. Output Head\n        x_flat = x_out.reshape(B, -1)\n        return self.head(x_flat)\n\n# ==============================================================================\n# TRAINING ENGINE\n# ==============================================================================\n\ndef train_model(model, train_loader, val_loader, epochs, lr, device):\n    model = model.to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n    criterion = nn.MSELoss()\n    scaler = torch.cuda.amp.GradScaler()\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n    \n    best_val_loss = float('inf')\n    patience = 0\n    start_time = time.time()\n    \n    for epoch in range(epochs):\n        model.train()\n        for bx, by in train_loader:\n            optimizer.zero_grad()\n            with torch.cuda.amp.autocast():\n                pred = model(bx)\n                loss = criterion(pred, by)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        \n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for bx, by in val_loader:\n                with torch.cuda.amp.autocast():\n                    pred = model(bx)\n                    loss = criterion(pred, by)\n                val_loss += loss.item()\n        \n        val_loss /= len(val_loader)\n        scheduler.step(val_loss)\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience = 0\n        else:\n            patience += 1\n            if patience >= 5:\n                break\n    \n    return model, time.time() - start_time\n\ndef evaluate_model(model, test_loader, device):\n    model.eval()\n    preds, targets = [], []\n    \n    start = time.time()\n    with torch.no_grad():\n        for bx, by in test_loader:\n            with torch.cuda.amp.autocast():\n                pred = model(bx)\n            preds.append(pred.cpu().numpy())\n            targets.append(by.cpu().numpy())\n    \n    inference_time = time.time() - start\n    preds = np.concatenate(preds)\n    targets = np.concatenate(targets)\n    \n    mse = mean_squared_error(targets, preds)\n    mae = mean_absolute_error(targets, preds)\n    rmse = np.sqrt(mse)\n    \n    return {\n        'MSE': mse,\n        'MAE': mae,\n        'RMSE': rmse,\n        'Inference_Time': inference_time,\n        'predictions': preds[:500]\n    }\n\n# ==============================================================================\n# BENCHMARK\n# ==============================================================================\n\ndef run_benchmark():\n    results = []\n    \n    model_configs = {\n        'LSTM': LSTM_Model,\n        'GRU': GRU_Model,\n        'Transformer': Transformer_Model,\n        'PatchTST': PatchTST_Model,\n        'FEDformer': FEDformer_Model,\n        'TSMixer': TSMixer_Model,\n        'iTransformer': iTransformer_Model,\n        'MTGNN': MTGNN_Model\n    }\n    \n    for dataset_name, data in datasets.items():\n        print(f\"\\n{'='*70}\")\n        print(f\"ðŸ“Š {dataset_name} | Shape: {data.shape}\")\n        print(f\"{'='*70}\")\n        \n        (train_x, train_y), (val_x, val_y), (test_x, test_y) = create_dataset(\n            data, CONFIG['SEQ_LEN'], CONFIG['PRED_LEN']\n        )\n        \n        train_loader = DataLoader(\n            TensorDataset(\n                torch.tensor(train_x, device=CONFIG['DEVICE']),\n                torch.tensor(train_y, device=CONFIG['DEVICE'])\n            ),\n            batch_size=CONFIG['BATCH_SIZE'], shuffle=True\n        )\n        \n        val_loader = DataLoader(\n            TensorDataset(\n                torch.tensor(val_x, device=CONFIG['DEVICE']),\n                torch.tensor(val_y, device=CONFIG['DEVICE'])\n            ),\n            batch_size=CONFIG['BATCH_SIZE'] * 2\n        )\n        \n        test_loader = DataLoader(\n            TensorDataset(\n                torch.tensor(test_x, device=CONFIG['DEVICE']),\n                torch.tensor(test_y, device=CONFIG['DEVICE'])\n            ),\n            batch_size=CONFIG['BATCH_SIZE'] * 2\n        )\n        \n        # Save test data\n        with open(CONFIG['SAVE_DIR'] / f'{dataset_name}_test.pkl', 'wb') as f:\n            pickle.dump({\n                'train_x': train_x, 'train_y': train_y,\n                'val_x': val_x, 'val_y': val_y,\n                'test_x': test_x, 'test_y': test_y,\n                'input_dim': data.shape[1]\n            }, f)\n        \n        for model_name, model_class in model_configs.items():\n            print(f\"âš¡ {model_name:12s} \", end='')\n            \n            try:\n                # Instantiate with correct args based on class signature\n                if model_name in ['DLinear', 'MTGNN']:\n                    model = model_class(data.shape[1], CONFIG['SEQ_LEN'], CONFIG['PRED_LEN'])\n                elif model_name in ['PatchTST', 'TSMixer', 'iTransformer']:\n                    model = model_class(data.shape[1], CONFIG['SEQ_LEN'], CONFIG['PRED_LEN'])\n                else:\n                    # LSTM, GRU, Transformer, FEDformer (standard signature)\n                    model = model_class(data.shape[1], CONFIG['PRED_LEN'])\n                \n                params = count_parameters(model)\n                \n                model, train_time = train_model(\n                    model, train_loader, val_loader,\n                    CONFIG['EPOCHS'], CONFIG['LR'], CONFIG['DEVICE']\n                )\n                \n                metrics = evaluate_model(model, test_loader, CONFIG['DEVICE'])\n                \n                results.append({\n                    'Dataset': dataset_name,\n                    'Model': model_name,\n                    'Params': params,\n                    'Train_Time': train_time,\n                    **metrics\n                })\n                \n                print(f\"âœ“ MSE={metrics['MSE']:.4f} | Params={params:,} | Time={train_time:.1f}s\")\n                \n                del model\n                torch.cuda.empty_cache()\n                \n            except Exception as e:\n                print(f\"âœ— Error: {str(e)}\")\n                import traceback\n                traceback.print_exc()\n    \n    # Save\n    df = pd.DataFrame(results)\n    df.to_csv(CONFIG['SAVE_DIR'] / 'baseline_results.csv', index=False)\n    \n    with open(CONFIG['SAVE_DIR'] / 'baseline_results.pkl', 'wb') as f:\n        pickle.dump(results, f)\n    \n    # Print summary\n    print(f\"\\n{'='*100}\")\n    print(f\"{'DATASET':<12} | {'MODEL':<12} | {'PARAMS':<8} | {'MSE':<10} | {'MAE':<10} | {'TRAIN(s)':<10}\")\n    print(f\"{'='*100}\")\n    \n    if not df.empty:\n        for _, row in df.iterrows():\n            print(f\"{row['Dataset']:<12} | {row['Model']:<12} | {row['Params']:<8} | \"\n                  f\"{row['MSE']:<10.4f} | {row['MAE']:<10.4f} | {row['Train_Time']:<10.1f}\")\n    \n    print(f\"{'='*100}\\n\")\n    \n    # Best per dataset\n    print(\"ðŸ† BEST MODELS:\")\n    if not df.empty:\n        for dataset in df['Dataset'].unique():\n            best = df[df['Dataset'] == dataset].nsmallest(1, 'MSE').iloc[0]\n            print(f\"  {dataset:12s}: {best['Model']:12s} (MSE={best['MSE']:.4f})\")\n    \n    print(f\"\\nâœ… Saved to {CONFIG['SAVE_DIR']}/baseline_results.csv\")\n    \n    return df\n\nif __name__ == '__main__':\n    run_benchmark()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T20:09:45.547421Z","iopub.execute_input":"2025-12-17T20:09:45.547805Z","iopub.status.idle":"2025-12-17T20:10:36.043142Z","shell.execute_reply.started":"2025-12-17T20:09:45.547782Z","shell.execute_reply":"2025-12-17T20:10:36.042272Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Device: Tesla P100-PCIE-16GB\nðŸŽ¯ Target: ~20,000 parameters per model\n\nLoading Electricity... âœ“\n\n======================================================================\nðŸ“Š Electricity | Shape: (15000, 12)\n======================================================================\nâš¡ LSTM         âœ“ MSE=0.1895 | Params=6,584 | Time=4.1s\nâš¡ GRU          âœ“ MSE=0.1400 | Params=6,441 | Time=5.1s\nâš¡ Transformer  âœ“ MSE=0.1020 | Params=34,296 | Time=16.6s\nâš¡ PatchTST     âœ“ MSE=0.0632 | Params=19,544 | Time=3.8s\nâš¡ FEDformer    âœ“ MSE=0.4025 | Params=6,224 | Time=7.4s\nâš¡ TSMixer      âœ“ MSE=0.0515 | Params=46,680 | Time=3.1s\nâš¡ iTransformer âœ“ MSE=0.0740 | Params=12,440 | Time=3.9s\nâš¡ MTGNN        âœ“ MSE=0.0539 | Params=18,880 | Time=5.8s\n\n====================================================================================================\nDATASET      | MODEL        | PARAMS   | MSE        | MAE        | TRAIN(s)  \n====================================================================================================\nElectricity  | LSTM         | 6584     | 0.1895     | 0.3543     | 4.1       \nElectricity  | GRU          | 6441     | 0.1400     | 0.3015     | 5.1       \nElectricity  | Transformer  | 34296    | 0.1020     | 0.2568     | 16.6      \nElectricity  | PatchTST     | 19544    | 0.0632     | 0.1999     | 3.8       \nElectricity  | FEDformer    | 6224     | 0.4025     | 0.5099     | 7.4       \nElectricity  | TSMixer      | 46680    | 0.0515     | 0.1808     | 3.1       \nElectricity  | iTransformer | 12440    | 0.0740     | 0.2172     | 3.9       \nElectricity  | MTGNN        | 18880    | 0.0539     | 0.1845     | 5.8       \n====================================================================================================\n\nðŸ† BEST MODELS:\n  Electricity : TSMixer      (MSE=0.0515)\n\nâœ… Saved to benchmark_results/baseline_results.csv\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"\"\"\"\nCausalGraphMemoryNet (CGMN) - Time Series Forecasting\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ==============================================================================\n# CONFIGURATION\n# ==============================================================================\nCONFIG = {\n    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu',\n    'SEQ_LEN': 96,\n    'PRED_LEN': 24,\n    'PATCH_SIZE': 16,\n    'SEEDS': [42, 123, 456],\n    'BATCH_SIZE': 512,\n    'EPOCHS': 100,\n    'LR': 0.002,\n    'HIDDEN_DIM': 32,\n    'N_HEADS': 4,\n    'N_LAYERS': 2,\n    'N_MEMORY': 8,\n    'GRAPH_DIM': 16,\n    'DROPOUT': 0.15,\n}\n\nBASELINE = {'mse': 0.0515}\n\n# ==============================================================================\n# DATA\n# ==============================================================================\ndef load_electricity(n=15000, seed=42):\n    np.random.seed(seed)\n    t = np.arange(n)\n    data = np.zeros((n, 12))\n    for i in range(12):\n        phase = i * 0.5\n        neighbor = 0.3 * np.sin(t * 2*np.pi / 24 + (i+1) % 12 * 0.5)\n        data[:, i] = (np.sin(t * 2*np.pi / 24 + phase) + \n                     1.5*np.sin(t * 2*np.pi / 168) + neighbor +\n                     np.linspace(0, 2, n) + np.random.normal(0, 0.3, n))\n    return StandardScaler().fit_transform(data).astype(np.float32)\n\ndef create_dataset(data, seq_len, pred_len):\n    X, y = [], []\n    for i in range(len(data) - seq_len - pred_len):\n        X.append(data[i:i+seq_len])\n        y.append(data[i+seq_len:i+seq_len+pred_len, 0])\n    X, y = np.array(X), np.array(y)\n    s1, s2 = int(0.7*len(X)), int(0.85*len(X))\n    return {\n        'train_x': X[:s1], 'train_y': y[:s1],\n        'val_x': X[s1:s2], 'val_y': y[s1:s2],\n        'test_x': X[s2:], 'test_y': y[s2:],\n        'input_dim': data.shape[1]\n    }\n\ndef create_loaders(data, config):\n    device = config['DEVICE']\n    return {\n        'train': DataLoader(\n            TensorDataset(\n                torch.tensor(data['train_x'], device=device).float(),\n                torch.tensor(data['train_y'], device=device).float()\n            ), batch_size=config['BATCH_SIZE'], shuffle=True, drop_last=True\n        ),\n        'val': DataLoader(\n            TensorDataset(\n                torch.tensor(data['val_x'], device=device).float(),\n                torch.tensor(data['val_y'], device=device).float()\n            ), batch_size=config['BATCH_SIZE']*2\n        ),\n        'test': DataLoader(\n            TensorDataset(\n                torch.tensor(data['test_x'], device=device).float(),\n                torch.tensor(data['test_y'], device=device).float()\n            ), batch_size=config['BATCH_SIZE']*2\n        ),\n    }\n\n# ==============================================================================\n# COMPONENT 1: Graph-Gated Channel Fusion (GGCF)\n# ==============================================================================\nclass GraphGatedChannelFusion(nn.Module):\n    def __init__(self, n_channels, hidden_dim, graph_dim=16):\n        super().__init__()\n        self.node_src = nn.Parameter(torch.randn(n_channels, graph_dim) * 0.1)\n        self.node_tgt = nn.Parameter(torch.randn(n_channels, graph_dim) * 0.1)\n        self.temp = nn.Parameter(torch.tensor(1.0))\n        \n        self.gate = nn.Sequential(\n            nn.Linear(n_channels * 2, n_channels),\n            nn.Sigmoid()\n        )\n        self.mix = nn.Sequential(\n            nn.Linear(n_channels, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.GELU(),\n            nn.Linear(hidden_dim, n_channels)\n        )\n        self.register_buffer('adj_matrix', torch.zeros(n_channels, n_channels))\n    \n    def forward(self, x):\n        adj = F.softmax(self.node_src @ self.node_tgt.T / (0.5 + torch.sigmoid(self.temp)), dim=-1)\n        self.adj_matrix = adj.detach()\n        \n        x_graph = torch.einsum('btc,cd->btd', x, adj)\n        gate = self.gate(torch.cat([x, x_graph], dim=-1))\n        return x + gate * self.mix(x_graph)\n\n# ==============================================================================\n# COMPONENT 2: Temporal Memory Bank (TMB)\n# ==============================================================================\nclass TemporalMemoryBank(nn.Module):\n    def __init__(self, dim, n_memory=8, n_heads=4):\n        super().__init__()\n        self.n_memory = n_memory\n        self.n_heads = n_heads\n        self.head_dim = dim // n_heads\n        self.scale = self.head_dim ** -0.5\n        \n        self.memory = nn.Parameter(torch.randn(n_memory, dim) * 0.02)\n        self.qkv = nn.Linear(dim, dim * 3)\n        self.mem_kv = nn.Linear(dim, dim * 2)\n        self.out = nn.Linear(dim, dim)\n        self.register_buffer('usage', torch.zeros(n_memory))\n    \n    def forward(self, x):\n        B, N, D = x.shape\n        \n        qkv = self.qkv(x).reshape(B, N, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        Q, K, V = qkv[0], qkv[1], qkv[2]\n        \n        mem = self.memory.unsqueeze(0).expand(B, -1, -1)\n        mem_kv = self.mem_kv(mem).reshape(B, self.n_memory, 2, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        K_mem, V_mem = mem_kv[0], mem_kv[1]\n        \n        K_full = torch.cat([K_mem, K], dim=2)\n        V_full = torch.cat([V_mem, V], dim=2)\n        \n        attn = (Q @ K_full.transpose(-2, -1)) * self.scale\n        \n        causal = torch.triu(torch.ones(N, N, device=x.device), 1).bool()\n        mask = torch.zeros(N, self.n_memory + N, device=x.device).bool()\n        mask[:, self.n_memory:] = causal\n        attn = attn.masked_fill(mask.unsqueeze(0).unsqueeze(0), torch.finfo(attn.dtype).min)\n        attn = F.softmax(attn, dim=-1)\n        \n        if self.training:\n            with torch.no_grad():\n                self.usage = 0.9 * self.usage + 0.1 * attn[:, :, :, :self.n_memory].mean(dim=(0,1,2))\n        \n        out = (attn @ V_full).transpose(1, 2).reshape(B, N, D)\n        return self.out(out)\n\n# ==============================================================================\n# COMPONENT 3: Progressive Weight Sharing (PWS)\n# ==============================================================================\nclass SharedWeightFFN(nn.Module):\n    def __init__(self, dim, expansion=4, n_templates=4, dropout=0.1):\n        super().__init__()\n        self.dim = dim\n        self.hidden = dim * expansion\n        self.n_templates = n_templates\n        \n        self.templates = nn.Parameter(torch.randn(n_templates, dim, dim) * 0.02)\n        self.selector = nn.Linear(dim, n_templates)\n        \n        self.fc1 = nn.Linear(dim, self.hidden)\n        self.fc2 = nn.Linear(self.hidden, dim)\n        self.dropout = nn.Dropout(dropout)\n        self.norm = nn.LayerNorm(dim)\n        \n        self.register_buffer('sharing_stats', torch.zeros(n_templates))\n    \n    def forward(self, x):\n        weights = F.softmax(self.selector(x.mean(dim=1)), dim=-1)\n        \n        if self.training:\n            with torch.no_grad():\n                self.sharing_stats = 0.9 * self.sharing_stats + 0.1 * weights.mean(0)\n        \n        shared = torch.einsum('bn,nde,bte->btd', weights, self.templates, x)\n        \n        h = F.gelu(self.fc1(x + shared))\n        out = self.fc2(self.dropout(h))\n        return self.norm(out)\n    \n    def get_sharing_ratio(self):\n        stats = self.sharing_stats.cpu()\n        entropy = -(stats * (stats + 1e-8).log()).sum()\n        return (1 - entropy / np.log(self.n_templates)).item()\n\n# ==============================================================================\n# ENCODER LAYER\n# ==============================================================================\nclass EncoderLayer(nn.Module):\n    def __init__(self, dim, n_memory=8, n_heads=4, dropout=0.1, use_memory=True, use_sharing=True):\n        super().__init__()\n        self.use_memory = use_memory\n        self.use_sharing = use_sharing\n        \n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.dropout = nn.Dropout(dropout)\n        \n        if use_memory:\n            self.attn = TemporalMemoryBank(dim, n_memory, n_heads)\n        else:\n            self.attn = nn.MultiheadAttention(dim, n_heads, dropout, batch_first=True)\n            self.register_buffer('causal_mask', torch.triu(torch.ones(96//16, 96//16), 1).bool())\n        \n        if use_sharing:\n            self.ffn = SharedWeightFFN(dim, 4, 4, dropout)\n        else:\n            self.ffn = nn.Sequential(\n                nn.Linear(dim, dim * 4),\n                nn.GELU(),\n                nn.Dropout(dropout),\n                nn.Linear(dim * 4, dim),\n                nn.LayerNorm(dim)\n            )\n    \n    def forward(self, x):\n        if self.use_memory:\n            x = x + self.dropout(self.attn(self.norm1(x)))\n        else:\n            x_norm = self.norm1(x)\n            attn_out, _ = self.attn(x_norm, x_norm, x_norm, attn_mask=self.causal_mask)\n            x = x + self.dropout(attn_out)\n        \n        x = x + self.dropout(self.ffn(self.norm2(x)))\n        return x\n\n# ==============================================================================\n# MAIN MODEL\n# ==============================================================================\nclass CausalGraphMemoryNet(nn.Module):\n    def __init__(self, input_dim, seq_len, pred_len, hidden_dim=32, \n                 n_heads=4, n_layers=2, patch_size=16, n_memory=8,\n                 graph_dim=16, dropout=0.1,\n                 use_graph=True, use_memory=True, use_sharing=True):\n        super().__init__()\n        \n        self.n_patches = seq_len // patch_size\n        self.patch_size = patch_size\n        self.use_graph = use_graph\n        self.use_memory = use_memory\n        self.use_sharing = use_sharing\n        \n        self.register_buffer('eps', torch.tensor(1e-5))\n        \n        if use_graph:\n            self.graph = GraphGatedChannelFusion(input_dim, hidden_dim, graph_dim)\n        \n        self.patch_embed = nn.Linear(patch_size * input_dim, hidden_dim)\n        self.pos_embed = nn.Parameter(torch.randn(1, self.n_patches, hidden_dim) * 0.02)\n        \n        self.layers = nn.ModuleList([\n            EncoderLayer(hidden_dim, n_memory, n_heads, dropout, use_memory, use_sharing)\n            for _ in range(n_layers)\n        ])\n        \n        self.norm = nn.LayerNorm(hidden_dim)\n        self.head = nn.Linear(hidden_dim * self.n_patches, pred_len)\n        self._init_weights()\n    \n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.trunc_normal_(m.weight, std=0.02)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n    \n    def forward(self, x):\n        B, T, C = x.shape\n        \n        self._mean = x.mean(dim=1, keepdim=True)\n        self._std = x.std(dim=1, keepdim=True) + self.eps\n        x = (x - self._mean) / self._std\n        \n        if self.use_graph:\n            x = self.graph(x)\n        \n        x = x[:, :self.n_patches * self.patch_size].reshape(B, self.n_patches, -1)\n        x = self.patch_embed(x) + self.pos_embed\n        \n        for layer in self.layers:\n            x = layer(x)\n        \n        x = self.norm(x).reshape(B, -1)\n        pred = self.head(x)\n        \n        return pred * self._std[:, 0, 0:1] + self._mean[:, 0, 0:1]\n    \n    def get_stats(self):\n        stats = {'params': sum(p.numel() for p in self.parameters())}\n        if self.use_graph:\n            adj = self.graph.adj_matrix.cpu()\n            stats['graph_sparsity'] = (adj < 0.1).float().mean().item()\n        if self.use_memory:\n            stats['memory_util'] = np.mean([l.attn.usage.cpu().mean().item() for l in self.layers])\n        if self.use_sharing:\n            ratios = [l.ffn.get_sharing_ratio() for l in self.layers if hasattr(l.ffn, 'get_sharing_ratio')]\n            stats['sharing'] = np.mean(ratios) if ratios else 0\n        return stats\n\n# ==============================================================================\n# TRAINING\n# ==============================================================================\ndef train_model(model, train_loader, val_loader, config, verbose=True):\n    device = config['DEVICE']\n    model.to(device)\n    \n    optimizer = optim.AdamW(model.parameters(), lr=config['LR'], weight_decay=0.01)\n    scheduler = optim.lr_scheduler.OneCycleLR(\n        optimizer, max_lr=config['LR'], epochs=config['EPOCHS'],\n        steps_per_epoch=len(train_loader), pct_start=0.1\n    )\n    \n    scaler = torch.cuda.amp.GradScaler()\n    best_val, best_state, patience = float('inf'), None, 0\n    \n    for epoch in range(config['EPOCHS']):\n        model.train()\n        train_loss = 0\n        for xb, yb in train_loader:\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast():\n                loss = F.mse_loss(model(xb), yb)\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n            train_loss += loss.item()\n        train_loss /= len(train_loader)\n        \n        model.eval()\n        val_loss = sum(F.mse_loss(model(xb), yb).item() for xb, yb in val_loader) / len(val_loader)\n        \n        if val_loss < best_val:\n            best_val = val_loss\n            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n            patience = 0\n        else:\n            patience += 1\n        \n        if verbose and epoch % 25 == 0:\n            print(f\"    Epoch {epoch+1:3d}: train={train_loss:.4f} val={val_loss:.4f}\")\n        \n        if patience >= 30:\n            break\n    \n    model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n    return model\n\ndef evaluate(model, loader):\n    model.eval()\n    preds, targets = [], []\n    with torch.no_grad():\n        for xb, yb in loader:\n            preds.append(model(xb).cpu().numpy())\n            targets.append(yb.cpu().numpy())\n    p, t = np.concatenate(preds), np.concatenate(targets)\n    return {'MSE': mean_squared_error(t, p), 'MAE': mean_absolute_error(t, p)}\n\n# ==============================================================================\n# ABLATION\n# ==============================================================================\ndef run_ablation(data, config):\n    print(\"\\n\" + \"=\"*60)\n    print(\"ABLATION STUDY\")\n    print(\"=\"*60)\n    \n    configs = {\n        'Full': {'use_graph': True, 'use_memory': True, 'use_sharing': True},\n        'w/o GGCF': {'use_graph': False, 'use_memory': True, 'use_sharing': True},\n        'w/o TMB': {'use_graph': True, 'use_memory': False, 'use_sharing': True},\n        'w/o PWS': {'use_graph': True, 'use_memory': True, 'use_sharing': False},\n    }\n    \n    results = []\n    loaders = create_loaders(data, config)\n    \n    for name, flags in configs.items():\n        torch.manual_seed(config['SEEDS'][0])\n        np.random.seed(config['SEEDS'][0])\n        \n        model = CausalGraphMemoryNet(\n            data['input_dim'], config['SEQ_LEN'], config['PRED_LEN'],\n            config['HIDDEN_DIM'], config['N_HEADS'], config['N_LAYERS'],\n            config['PATCH_SIZE'], config['N_MEMORY'], config['GRAPH_DIM'],\n            config['DROPOUT'], **flags\n        )\n        model = train_model(model, loaders['train'], loaders['val'], config, verbose=False)\n        m = evaluate(model, loaders['test'])\n        results.append({'Config': name, 'MSE': m['MSE'], 'MAE': m['MAE']})\n        print(f\"  {name:12s}: MSE={m['MSE']:.4f}\")\n    \n    df = pd.DataFrame(results)\n    full_mse = df[df['Config'] == 'Full']['MSE'].values[0]\n    \n    print(\"\\nComponent Impact:\")\n    for _, r in df[df['Config'] != 'Full'].iterrows():\n        delta = ((r['MSE'] - full_mse) / full_mse) * 100\n        print(f\"  {r['Config']:12s}: {delta:+.2f}%\")\n    \n    return df\n\n# ==============================================================================\n# MAIN\n# ==============================================================================\ndef run():\n    print(\"=\"*60)\n    print(\"CausalGraphMemoryNet (CGMN)\")\n    print(\"=\"*60)\n    \n    # Load data once\n    data = create_dataset(load_electricity(), CONFIG['SEQ_LEN'], CONFIG['PRED_LEN'])\n    \n    print(\"\\nBENCHMARK (3 seeds)\")\n    print(\"-\"*60)\n    \n    all_results = []\n    \n    for seed in CONFIG['SEEDS']:\n        print(f\"\\nSeed {seed}:\")\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n        \n        loaders = create_loaders(data, CONFIG)\n        \n        model = CausalGraphMemoryNet(\n            data['input_dim'], CONFIG['SEQ_LEN'], CONFIG['PRED_LEN'],\n            CONFIG['HIDDEN_DIM'], CONFIG['N_HEADS'], CONFIG['N_LAYERS'],\n            CONFIG['PATCH_SIZE'], CONFIG['N_MEMORY'], CONFIG['GRAPH_DIM'],\n            CONFIG['DROPOUT']\n        )\n        \n        if seed == CONFIG['SEEDS'][0]:\n            print(f\"  Params: {sum(p.numel() for p in model.parameters()):,}\")\n        \n        model = train_model(model, loaders['train'], loaders['val'], CONFIG)\n        m = evaluate(model, loaders['test'])\n        \n        imp = ((BASELINE['mse'] - m['MSE']) / BASELINE['mse']) * 100\n        print(f\"  MSE: {m['MSE']:.4f} ({imp:+.1f}%)\")\n        \n        all_results.append({'Seed': seed, 'MSE': m['MSE'], 'MAE': m['MAE'], 'Improvement': imp})\n    \n    df = pd.DataFrame(all_results)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"RESULTS SUMMARY\")\n    print(\"=\"*60)\n    print(f\"  MSE: {df['MSE'].mean():.4f} Â± {df['MSE'].std():.4f}\")\n    print(f\"  MAE: {df['MAE'].mean():.4f} Â± {df['MAE'].std():.4f}\")\n    print(f\"  Improvement: {df['Improvement'].mean():+.1f}% Â± {df['Improvement'].std():.1f}%\")\n    print(f\"  Baseline: {BASELINE['mse']:.4f}\")\n    \n    # Ablation\n    abl_df = run_ablation(data, CONFIG)\n    \n    # Model Analysis\n    print(\"\\n\" + \"=\"*60)\n    print(\"MODEL ANALYSIS\")\n    print(\"=\"*60)\n    \n    torch.manual_seed(CONFIG['SEEDS'][0])\n    model = CausalGraphMemoryNet(\n        data['input_dim'], CONFIG['SEQ_LEN'], CONFIG['PRED_LEN'],\n        CONFIG['HIDDEN_DIM'], CONFIG['N_HEADS'], CONFIG['N_LAYERS'],\n        CONFIG['PATCH_SIZE'], CONFIG['N_MEMORY'], CONFIG['GRAPH_DIM'],\n        CONFIG['DROPOUT']\n    )\n    loaders = create_loaders(data, CONFIG)\n    model = train_model(model, loaders['train'], loaders['val'], CONFIG, verbose=False)\n    \n    stats = model.get_stats()\n    print(f\"  Parameters: {stats['params']:,}\")\n    print(f\"  Graph sparsity: {stats.get('graph_sparsity', 0)*100:.1f}%\")\n    print(f\"  Memory utilization: {stats.get('memory_util', 0)*100:.1f}%\")\n    print(f\"  Weight sharing: {stats.get('sharing', 0)*100:.1f}%\")\n    \n    return {'benchmark': df, 'ablation': abl_df}\n\nif __name__ == \"__main__\":\n    results = run()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T20:10:45.407528Z","iopub.execute_input":"2025-12-17T20:10:45.407901Z","iopub.status.idle":"2025-12-17T20:16:09.401835Z","shell.execute_reply.started":"2025-12-17T20:10:45.407880Z","shell.execute_reply":"2025-12-17T20:16:09.401094Z"}},"outputs":[{"name":"stdout","text":"============================================================\nCausalGraphMemoryNet (CGMN)\n============================================================\n\nBENCHMARK (3 seeds)\n------------------------------------------------------------\n\nSeed 42:\n  Params: 51,353\n    Epoch   1: train=1.1290 val=0.9029\n    Epoch  26: train=0.0413 val=0.0409\n    Epoch  51: train=0.0403 val=0.0401\n    Epoch  76: train=0.0399 val=0.0399\n  MSE: 0.0417 (+19.1%)\n\nSeed 123:\n    Epoch   1: train=1.1007 val=0.8474\n    Epoch  26: train=0.0416 val=0.0407\n    Epoch  51: train=0.0404 val=0.0403\n    Epoch  76: train=0.0400 val=0.0399\n  MSE: 0.0418 (+18.8%)\n\nSeed 456:\n    Epoch   1: train=1.1291 val=0.8405\n    Epoch  26: train=0.0413 val=0.0406\n    Epoch  51: train=0.0404 val=0.0410\n    Epoch  76: train=0.0399 val=0.0401\n  MSE: 0.0418 (+18.8%)\n\n============================================================\nRESULTS SUMMARY\n============================================================\n  MSE: 0.0418 Â± 0.0001\n  MAE: 0.1630 Â± 0.0002\n  Improvement: +18.9% Â± 0.1%\n  Baseline: 0.0515\n\n============================================================\nABLATION STUDY\n============================================================\n  Full        : MSE=0.0417\n  w/o GGCF    : MSE=0.0417\n  w/o TMB     : MSE=0.0419\n  w/o PWS     : MSE=0.0417\n\nComponent Impact:\n  w/o GGCF    : +0.16%\n  w/o TMB     : +0.43%\n  w/o PWS     : +0.14%\n\n============================================================\nMODEL ANALYSIS\n============================================================\n  Parameters: 51,353\n  Graph sparsity: 68.8%\n  Memory utilization: 7.2%\n  Weight sharing: 0.1%\n","output_type":"stream"}],"execution_count":28}]}